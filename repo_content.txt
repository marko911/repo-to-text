Repository Content Extraction
Generated on: SystemTime { tv_sec: 1746775419, tv_nsec: 23850000 }
=================================================

fficient command-line tool that converts an entire code repository into a single text file, making it easy to feed your codebase into LLMs (Large Language Models).
Asking questions of your codebase to big LLM providers is surprisingly tricky because of file number limits and size limits. This tool is to help you get around that.

## Features

- üöÄ Fast parallel processing using Rayon
- üßπ Intelligent filtering of binary and non-source files
- üîç Smart handling of binary data blocks in source files
- üìù Clean, formatted output with file separators
- üõ†Ô∏è Configurable file extension filtering

## Installation

### From Source

```bash
# Clone the repository
git clone https://github.com/marko911/repo_to_text
cd repo_to_text

# Install to /usr/local/bin (requires sudo)
cargo install --path . --root /usr/local
```

### Manual Installation

```bash
# Build the release binary
cargo build --release

# Copy to /usr/local/bin (requires sudo)
sudo cp target/release/repo_to_text /usr/local/bin/
```

## Usage

Basic usage (run from within your repository):

```bash
repo_to_text
```

The tool will create a `repo_content.txt` file in the current directory containing all your repository's text content.

### Ignoring Additional File Extensions

You can specify additional file extensions to ignore:

```bash
repo_to_text --ignore txt,md,conf

# Or using the short form
repo_to_text -i txt,md,conf
```

### Default Ignored Content

- **Directories**:

  - `.git`
  - `.svn`
  - `node_modules`
  - `vendor`
  - `.idea`
  - `target`
  - `dist`
  - `build`
  - `.next`
  - `coverage`
  - `__pycache__`
  - `.pytest_cache`

- **File Extensions**:

  - Binary files: `exe`, `dll`, `so`, `dylib`, `bin`
  - Archives: `zip`, `tar`, `gz`, `rar`, `7z`
  - Images: `jpg`, `jpeg`, `png`, `gif`, `bmp`, `ico`, `svg`
  - Audio/Video: `mp3`, `mp4`, `wav`, `avi`, `mov`
  - Documents: `pdf`, `doc`, `docx`
  - Database: `db`, `sqlite`, `sqlite3`
  - Compiled: `pyc`, `class`, `o`
  - Package files: `lock`, `sum`

- **Special Files**:
  - macOS system files (starting with `._`)
  - Files without extensions
  - `.DS_Store`
  - `.env`
  - `.log`

## Output Format

The generated `repo_content.txt` file follows this format:

```
Repository Content Extraction
Generated on: 2024-12-22 08:13:01

===============================================
--- File: src/main.rs ---
===============================================
[file content here]
--- End of File ---
===============================================
```

## Use Cases

- Training custom LLMs on your codebase
- Creating context for LLM prompts
- Code analysis and documentation

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

--- End of File ---

===============================================
===============================================
--- File: ./Cargo.toml ---
===============================================

[package]
name = "repo_to_text"
version = "0.1.0"
edition = "2021"

[dependencies]
md5 = "0.7.0"
rayon = "1.8"
regex = "1.10"
tempfile = "3.9"
clap = { version = "4.4", features = ["derive"] }
dialoguer = "0.11.0"

--- End of File ---

===============================================
===============================================
--- File: ./src/main.rs ---
===============================================

use clap::Parser;
use dialoguer::{theme::ColorfulTheme, MultiSelect};
use rayon::prelude::*;
use regex::Regex;
use std::{
    collections::HashSet,
    fs::{self, File},
    io::{self, BufWriter, Write},
    path::{Path, PathBuf},
    sync::{Arc, Mutex},
    time::SystemTime,
};

#[derive(Parser)]
#[command(author, version, about, long_about = None)]
struct Args {
    /// Additional items to ignore (both directories and file extensions). Can be space or comma separated.
    #[arg(short, long, value_delimiter = ',', num_args = 1..)]
    ignore: Option<Vec<String>>,
}

struct RepoProcessor {
    output_file: String,
    ignored_dirs: HashSet<String>,
    ignored_exts: HashSet<String>,
    temp_dir: PathBuf,
    large_files: Arc<Mutex<Vec<(PathBuf, u64)>>>,
    size_threshold: u64,
}

impl RepoProcessor {
    fn new(additional_ignores: Option<Vec<String>>) -> io::Result<Self> {
        let temp_dir = tempfile::tempdir()?.into_path();

        let mut ignored_dirs: HashSet<String> =
            vec!["git", "svn", "node_modules", "vendor", "idea", "target"]
                .into_iter()
                .map(String::from)
                .collect();

        // Match the fish script's ignored extensions exactly
        let mut ignored_exts: HashSet<String> = vec![
            "lock",
            "pack",
            "xz",
            "7z",
            "bz2",
            "gz",
            "lz",
            "lzma",
            "lzo",
            "rar",
            "tar",
            "xz",
            "z",
            "zip",
            "deb",
            "rpm",
            "apk",
            "ipa",
            "app",
            "dmg",
            "pkg",
            "exe",
            "dll",
            "csv",
            "so",
            "o",
            "a",
            "pyc",
            "class",
            "jar",
            "war",
            "woff",
            "woff2",
            "ttf",
            "eot",
            "env",
            "log",
            "gitignore",
            "json",
            "npmrc",
            "prettierrc",
            "eslintrc",
            "babelrc",
            "pyc",
            "pyo",
            "pyd",
            "class",
            "yml",
            "yaml",
            "jpg",
            "jpeg",
            "png",
            "gif",
            "bmp",
            "ico",
            "svg",
            "webp",
            "pdf",
            "bcmap", // Binary CMap files for PDF/font processing
            "pfb",   // Printer Font Binary
            "pfm",   // Printer Font Metrics
            "afm",   // Adobe Font Metrics
            "otf",   // OpenType Font
            "cff",   // Compact Font Format
            "fon",   // Legacy Windows Font format
        ]
        .into_iter()
        .map(String::from)
        .collect();

        // Add user-provided extensions if any
        if let Some(additional) = additional_ignores {
            for item in additional {
                let clean_item = item.trim_start_matches('.');
                ignored_exts.insert(clean_item.to_string());
                ignored_dirs.insert(clean_item.to_string());
            }
        }

        Ok(Self {
            output_file: "repo_content.txt".to_string(),
            ignored_dirs,
            ignored_exts,
            temp_dir,
            large_files: Arc::new(Mutex::new(Vec::new())),
            size_threshold: 1024 * 1024, // 1MB in bytes
        })
    }

    fn should_ignore_dir(&self, dir: &str) -> bool {
        let dir_clean = dir.trim_start_matches('.');
        self.ignored_dirs.contains(dir) || self.ignored_dirs.contains(dir_clean)
    }

    fn should_ignore_ext(&self, file: &Path) -> bool {
        let filename = file
            .file_name()
            .map(|f| f.to_string_lossy())
            .unwrap_or_default();

        // Check if file has no extension
        if !filename.contains('.') || filename.ends_with('.') {
            return true;
        }

        // Check for .so.* pattern (case-insensitive)
        if filename.to_lowercase().contains(".so.") {
            return true;
        }

        let extension = match file.extension() {
            Some(ext) => ext.to_string_lossy().to_string().to_lowercase(),
            None => return true,
        };

        self.ignored_exts.contains(&extension)
    }

    fn collect_files(&self, dir: &Path) -> io::Result<Vec<PathBuf>> {
        let mut files = Vec::new();
        let mut large_files = Vec::new();

        if dir.is_dir() {
            for entry in fs::read_dir(dir)? {
                let entry = entry?;
                let path = entry.path();

                if path.is_dir() {
                    if let Some(dirname) = path.file_name() {
                        if !self.should_ignore_dir(&dirname.to_string_lossy()) {
                            files.extend(self.collect_files(&path)?);
                        }
                    }
                } else if path.is_file() {
                    if let Some(filename) = path.file_name() {
                        if !filename.to_string_lossy().starts_with("._")
                            && !self.should_ignore_ext(&path)
                        {
                            // Check file size
                            if let Ok(metadata) = path.metadata() {
                                let size = metadata.len();
                                if size > self.size_threshold {
                                    large_files.push((path.clone(), size));
                                }
                            }
                            files.push(path);
                        }
                    }
                }
            }
        }

        // Store large files in the struct
        if !large_files.is_empty() {
            self.large_files.lock().unwrap().extend(large_files);
        }

        Ok(files)
    }

    fn process_file(&self, file: &Path) -> io::Result<PathBuf> {
        let hash = format!("{:x}", md5::compute(file.to_string_lossy().as_bytes()));
        let outfile = self.temp_dir.join(format!("{}.txt", hash));

        let mut writer = BufWriter::new(File::create(&outfile)?);

        writeln!(writer, "===============================================")?;
        writeln!(writer, "--- File: {} ---", file.display())?;
        writeln!(writer, "===============================================")?;
        writeln!(writer)?;

        // Read file as bytes instead of UTF-8 string
        let content = fs::read(file)?;

        // Convert to string, replacing invalid UTF-8 with placeholder
        let content = String::from_utf8_lossy(&content);

        // Process content with the same patterns as the fish script
        let binary_patterns = [
            (
                r#"(?s)(DATA = b""")[^"]*?(""")"#,
                r#"$1<binary data removed>$2"#,
            ),
            (
                r#"(?s)(b85decode\().*?(\))"#,
                r#"$1"<binary data removed>"$2"#,
            ),
            (
                r#"(?s)(base64\.[^(]*decode\().*?(\))"#,
                r#"$1"<binary data removed>"$2"#,
            ),
        ];

        let processed_content =
            binary_patterns
                .iter()
                .fold(content.to_string(), |acc, (pattern, replacement)| {
                    Regex::new(pattern)
                        .unwrap()
                        .replace_all(&acc, *replacement)
                        .to_string()
                });

        write!(writer, "{}", processed_content)?;
        writeln!(writer)?;
        writeln!(writer, "--- End of File ---")?;
        writeln!(writer)?;
        writeln!(writer, "===============================================")?;

        Ok(outfile)
    }

    fn prompt_large_files(&self, files: &[PathBuf]) -> io::Result<Vec<PathBuf>> {
        if self.large_files.lock().unwrap().is_empty() {
            return Ok(files.to_vec());
        }

        println!("\nFound large files (>1MB). Use ‚Üë‚Üì to navigate, Y/N to select, Enter when done:");
        let items: Vec<String> = self
            .large_files
            .lock()
            .unwrap()
            .iter()
            .map(|(path, size)| {
                format!(
                    "{} ({:.2}MB)",
                    path.display(),
                    *size as f64 / (1024.0 * 1024.0)
                )
            })
            .collect();

        let theme = ColorfulTheme::default();
        let mut current_selection = vec![true; items.len()];
        let mut current_index = 0;

        loop {
            // Clear screen and show current state
            print!("\x1B[2J\x1B[1;1H");
            println!("Select files to include (Y/N for current item, ‚Üë‚Üì to navigate, Enter to finish):\n");

            for (idx, item) in items.iter().enumerate() {
                let prefix = if idx == current_index { ">" } else { " " };
                let status = if current_selection[idx] { "Y" } else { "N" };
                println!("{} [{}] {}", prefix, status, item);
            }

            // Get user input
            if let Ok(key) = dialoguer::console::Term::stdout().read_key() {
                match key {
                    dialoguer::console::Key::Char('y') | dialoguer::console::Key::Char('Y') => {
                        current_selection[current_index] = true;
                        if current_index < items.len() - 1 {
                            current_index += 1;
                        }
                    }
                    dialoguer::console::Key::Char('n') | dialoguer::console::Key::Char('N') => {
                        current_selection[current_index] = false;
                        if current_index < items.len() - 1 {
                            current_index += 1;
                        }
                    }
                    dialoguer::console::Key::ArrowUp if current_index > 0 => {
                        current_index -= 1;
                    }
                    dialoguer::console::Key::ArrowDown if current_index < items.len() - 1 => {
                        current_index += 1;
                    }
                    dialoguer::console::Key::Enter => {
                        break;
                    }
                    _ => {}
                }
            }
        }

        let selected_paths: HashSet<_> = self
            .large_files
            .lock()
            .unwrap()
            .iter()
            .enumerate()
            .filter(|(i, _)| current_selection[*i])
            .map(|(_, (path, _))| path.clone())
            .collect();

        Ok(files
            .iter()
            .filter(|f| {
                if self
                    .large_files
                    .lock()
                    .unwrap()
                    .iter()
                    .any(|(p, _)| p == *f)
                {
                    selected_paths.contains(*f)
                } else {
                    true
                }
            })
            .cloned()
            .collect())
    }

    pub fn process_repository(&self) -> io::Result<()> {
        let mut output = BufWriter::new(File::create(&self.output_file)?);

        writeln!(output, "Repository Content Extraction")?;
        writeln!(output, "Generated on: {:?}", SystemTime::now())?;
        writeln!(output, "=================================================")?;
        writeln!(output)?;

        println!("Collecting files...");
        let files = self.collect_files(Path::new("."))?;

        // Prompt for large files before processing
        let files_to_process = self.prompt_large_files(&files)?;
        let total_files = files_to_process.len();

        println!("Processing {} files...", total_files);
        let processed_count = Arc::new(Mutex::new(0));
        let output_mutex = Arc::new(Mutex::new(BufWriter::new(File::create(&self.output_file)?)));

        // Process files in parallel using rayon's parallel iterator
        files_to_process
            .par_iter()
            .try_for_each(|file| -> io::Result<()> {
                let count = {
                    let mut count = processed_count.lock().unwrap();
                    *count += 1;
                    *count
                };

                print!(
                    "\rProcessing file {} of {}: {}",
                    count,
                    total_files,
                    file.display()
                );
                io::stdout().flush()?;

                let temp_file = self.process_file(file)?;
                let content = fs::read_to_string(&temp_file)?;

                // Write directly to the output file under lock
                let mut output = output_mutex.lock().unwrap();
                write!(output, "{}", content)?;

                // Clean up temp file immediately
                fs::remove_file(temp_file)?;

                Ok(())
            })?;

        println!(
            "\nFinished processing. Output saved to {}",
            self.output_file
        );

        // Cleanup temp directory
        fs::remove_dir_all(&self.temp_dir)?;

        Ok(())
    }
}

fn main() -> io::Result<()> {
    let args = Args::parse();
    let processor = RepoProcessor::new(args.ignore)?;
    processor.process_repository()
}

--- End of File ---

===============================================
===============================================
--- File: ./repo_content.txt ---
===============================================


--- End of File ---

===============================================
